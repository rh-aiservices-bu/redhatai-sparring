# üö™ Red Hat AI Sparring Program - Yellow Belt

![matrix](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExeWQ5YmVhMDVoMHZnemluNTlnbDIzcnBjMW81aHozcnh3MW9qMG51MCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/sULKEgDMX8LcI/giphy.gif)![matrix](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExeWQ5YmVhMDVoMHZnemluNTlnbDIzcnBjMW81aHozcnh3MW9qMG51MCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/sULKEgDMX8LcI/giphy.gif)![matrix](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExeWQ5YmVhMDVoMHZnemluNTlnbDIzcnBjMW81aHozcnh3MW9qMG51MCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/sULKEgDMX8LcI/giphy.gif)

Welcome to the Level 4 - Yellow Belt. Let's start! 

1. Log in OpenShift AI [here](http://red.ht/rhoai-bu-cluster).

2. Find your Data Science Project.

    ![neomspace.png](./images/neomspace.png)

## üíä Create Connection

1. First, let‚Äôs create a connection that is pointing to our teeny tiiinny LLM üíö In your Data Science Project, go to `Connections` , hit `Create Connection` and  select `URI - v1`.

    ![connection-uri.png](./images/connection-uri.png)

2. Fill out the form with below information:

	**Name:** `tinyllama 1.0 on quay.io`

    **URI:** `oci://quay.io/rh-aiservices-bu/tinyllama:1.0`

    <img src= "./images/connection-uri-2.png" alt="your-image-description" style="border: 2px solid grey;">

    After you hit `Create`, you should see a screen like this:

    <img src= "./images/connection-uri-3.png" alt="your-image-description" style="border: 2px solid grey;">


## üî¥ Deploy the Tiny Model

1. Now,  let‚Äôs go to `Models`, and select `Single Model Serving` if you haven‚Äôt picked yet.

    ![single-model-serving.png](./images/single-model-serving.png)

2. Then click `Deploy`.

    ![deploy-model.png](./images/deploy-model.png)

3. Fill out the form as below:

    - **Model deployment name:** `tinyllm`

    - **Serving Runtime:** `CUSTOM - vLLM CPU`

   -  **Model Serving Size:** `Small`

    - **Model route:**
        - Select Make deployed models available through an external route
        - Uncheck `Require token authentication` for now

    - **Source Model Location:**

        - Select the Connection you just created: `tinyllama 1.0 on quay.io`
  
    ..leave the rest as it is and hit `Deploy`

    <img src= "./images/deploy-model-2.png" alt="your-image-description" style="border: 2px solid grey;">
    <img src= "./images/deploy-model-3.png" alt="your-image-description" style="border: 2px solid grey;">

    After a little waiting, you should get a ‚òòÔ∏èGreen‚òòÔ∏è status üéâüéâ

    Congratulation, you just deployed a tiny but mighty model!

## üîµ re-Configure AnythingLLM

1. And now if you want to chat with your small but mighty model, all you need to do is make your AnythingLLM pointing to this model! First, let's copy the **external** endpoint of our model from `Models` page:

    ![model-external-endpoint.png](./images/model-external-endpoint.png)


2. Go back to your AnythingLLM settings by clicking the below icon on the right bottom corner:

    <img src= "./images/anythingllm-settings.png" alt="your-image-description" style="border: 2px solid grey;">


3. And Click `LLM`. Just replace the Granite endpoint URL with your TinyLLM endpoint URL:

    - **Base URL:** `<YOUR MODEL ENDPOINT>/v1`
  
      - For example: https://tinyllm-neo-space.apps.prod.mycluster.com/v1 (make sure you add **/v1** at the end.)

    - **API Key:** Please delete, it should be empty.

   -  **Chat Model Name:** `/mnt/models`

   -  **Max Tokens**: `512`

    Then **save changes**, and **go back**!

    ![anythingllm-settings-2.png](./images/anythingllm-settings-2.png)

## üï∂Ô∏è Test YOUR Tiny LLM

1. Have a little chat with it (and never take the power of GPUs for granted againüòÖ)

    ![anythingllm.gif](./images/anythingllm.gif)

